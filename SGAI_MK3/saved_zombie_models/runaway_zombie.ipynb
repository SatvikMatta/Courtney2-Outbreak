{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zombie_model():\n",
    "    \"\"\"\n",
    "    makes the model that will be used for zombies\n",
    "    The output of the model will be the predicted q value\n",
    "    for being in a certain state.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(INPUT_SHAPE))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(36 * 2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 8))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 64))\n",
    "    model.add(layers.SyncBatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(8 * 36))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    optimizer = keras.optimizers.Adam(0.002)\n",
    "    loss = keras.losses.Huber()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZombieEnvironment:\n",
    "    ACTION_SPACE = tuple(range(8))\n",
    "    ACTION_MAPPINGS = {\n",
    "        0: \"moveUp\",\n",
    "        1: \"moveDown\",\n",
    "        2: \"moveLeft\",\n",
    "        3: \"moveRight\",\n",
    "        4: \"biteUp\",\n",
    "        5: \"biteDown\",\n",
    "        6: \"biteLeft\",\n",
    "        7: \"biteRight\",\n",
    "    }\n",
    "    SIZE = (6, 6)\n",
    "\n",
    "    def __init__(self, max_timesteps: int = 300, have_enemy_player:bool=True, logdir: str = \"\", run_name=\"\") -> None:\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.reset()\n",
    "        self.total_timesteps = 0\n",
    "        self.total_invalid_moves = 0\n",
    "        self.writer = None\n",
    "        if logdir != \"\" and run_name != \"\":\n",
    "            self.writer = tf.summary.create_file_writer(f\"{logdir}/{run_name}\")\n",
    "        self.have_enemy_player = have_enemy_player\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = Board(ZombieEnvironment.SIZE, \"Zombie\")\n",
    "        self.board.populate(num_zombies=1)\n",
    "        self.enemyPlayer = GovernmentPlayer()\n",
    "        self.done = False\n",
    "\n",
    "        # coordinates of the first zombie\n",
    "        self.agentPosition = self.board.indexOf(True)\n",
    "\n",
    "        # useful for metrics\n",
    "        self.max_number_of_zombies = 1\n",
    "        self.episode_invalid_actions = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_timesteps = 0\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        action_name = ZombieEnvironment.ACTION_MAPPINGS[action]\n",
    "        if \"move\" in action_name:\n",
    "            valid, new_pos = self.board.actionToFunction[action_name](\n",
    "                self.board.toCoord(self.agentPosition)\n",
    "            )\n",
    "            if valid:\n",
    "                self.agentPosition = new_pos\n",
    "        else:  # bite variation\n",
    "            dest_coord = list(self.board.toCoord(self.agentPosition))\n",
    "            if action_name == \"biteUp\":\n",
    "                dest_coord[1] -= 1\n",
    "            elif action_name == \"biteDown\":\n",
    "                dest_coord[1] += 1\n",
    "            elif action_name == \"biteRight\":\n",
    "                dest_coord[0] += 1\n",
    "            else:\n",
    "                dest_coord[0] -= 1\n",
    "            valid, _ = self.board.actionToFunction[\"bite\"](dest_coord)\n",
    "\n",
    "        won = None\n",
    "        # do the opposing player's action if the action was valid.\n",
    "        if valid:\n",
    "            _action, coord = self.enemyPlayer.get_move(self.board)\n",
    "            if not _action:\n",
    "                self.done = True\n",
    "                won = True\n",
    "            else:\n",
    "                if self.have_enemy_player:\n",
    "                    self.board.actionToFunction[_action](coord)\n",
    "            self.board.update()\n",
    "\n",
    "        # see if the game is over\n",
    "        if not self.board.States[\n",
    "            self.agentPosition\n",
    "        ].person.isZombie:  # zombie was cured\n",
    "            self.done = True\n",
    "            won = False\n",
    "        if not self.board.is_move_possible_at(self.agentPosition):  # no move possible\n",
    "            self.done = True\n",
    "        if self.episode_timesteps > self.max_timesteps:\n",
    "            self.done = True\n",
    "        if not valid:\n",
    "            self.done = True\n",
    "\n",
    "        # get obs, reward, done, info\n",
    "        obs, reward, done, info = (\n",
    "            self._get_obs(),\n",
    "            self._get_reward(action_name, valid, won),\n",
    "            self._get_done(),\n",
    "            self._get_info(),\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.episode_reward += reward\n",
    "        if not valid:\n",
    "            self.episode_invalid_actions += 1\n",
    "            self.total_invalid_moves += 1\n",
    "        self.episode_timesteps += 1\n",
    "        self.max_number_of_zombies = max(\n",
    "            self.board.num_zombies(), self.max_number_of_zombies\n",
    "        )\n",
    "        self.total_timesteps += 1\n",
    "\n",
    "        # write the metrics\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"train/invalid_action_rate\",\n",
    "                    self.total_invalid_moves / self.total_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\"train/cur_reward\", reward, step=self.total_timesteps)\n",
    "\n",
    "        # return the obs, reward, done, info\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_done(self):\n",
    "        return self.done\n",
    "\n",
    "    def _get_reward(self, action_name: str, was_valid: bool, won: bool):\n",
    "        \"\"\"\n",
    "        Gonna try to return reward between [-1, 1]\n",
    "        This fits w/i tanh and sigmoid ranges\n",
    "        \"\"\"\n",
    "        if not was_valid:\n",
    "            return -1\n",
    "        if won is True:\n",
    "            return 1\n",
    "        if won is False:\n",
    "            return -0.5\n",
    "        if \"bite\" in action_name:\n",
    "            return 0.3\n",
    "        return 0.01  # this is the case where it was move\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Is based off the assumption that 5 is not in the returned board.\n",
    "        Uses 5 as the key for current position.\n",
    "        \"\"\"\n",
    "        AGENT_POSITION_CONSTANT = 5\n",
    "        ret = self.board.get_board()\n",
    "        ret[self.agentPosition] = AGENT_POSITION_CONSTANT\n",
    "\n",
    "        # normalize observation to be be centered at 0\n",
    "        ret = np.array(ret, dtype=np.float32)\n",
    "        ret /= np.float32(AGENT_POSITION_CONSTANT)\n",
    "        ret -= np.float32(0.5)\n",
    "        return ret\n",
    "\n",
    "    def render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.run(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def init_render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.initScreen(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        import pygame\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "    def write_run_metrics(self):\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/num_invalid_actions_per_ep\",\n",
    "                    self.episode_invalid_actions,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_length\",\n",
    "                    self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_total_reward\",\n",
    "                    self.episode_reward,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/mean_reward\",\n",
    "                    self.episode_reward / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/percent_invalid_per_ep\",\n",
    "                    self.episode_invalid_actions / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.999\n",
    "EPSILON_MAX = 0.9  # exploration rate maximum\n",
    "EPSILON_MIN = 0.05  # exploration rate minimum\n",
    "EPS_DECAY = 1000  # decay rate, in steps\n",
    "TARGET_UPDATE = 500  # how many episodes before the target is updated\n",
    "\n",
    "BUFFER_CAPACITY = 10000\n",
    "memory = ReplayMemory(BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_zombie_action(state, steps_done: int = -1, writer=None):\n",
    "    \"\"\"\n",
    "    If no steps are provided, assuming not going to do\n",
    "    random exploration\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    eps_threshold = 0\n",
    "    if steps_done != -1:\n",
    "        eps_threshold = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN) * math.exp(\n",
    "            -1.0 * steps_done / EPS_DECAY\n",
    "        )\n",
    "    if writer is not None:\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(\"exploration rate\", eps_threshold, step=steps_done)\n",
    "    if sample > eps_threshold:\n",
    "        # Pick the action with the largest expected reward.\n",
    "        temp = zombie_policy(state, training=False)\n",
    "        numpy = temp.numpy().flatten()\n",
    "        if writer is not None:\n",
    "            with writer.as_default():\n",
    "                for idx, name in ZombieEnvironment.ACTION_MAPPINGS.items():\n",
    "                    tf.summary.scalar(f\"q_vals/{name}\", numpy[idx], step=steps_done)\n",
    "        return tf.constant([tuple(numpy).index(max(numpy))], dtype=tf.int32)\n",
    "    else:\n",
    "        return tf.constant([random.randrange(ZOMBIE_OUTPUT_SIZE)], dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_on_batch(\n",
    "    state_batch: tf.Tensor,\n",
    "    action_batch: tf.Tensor,\n",
    "    reward_batch: tf.Tensor,\n",
    "    non_final_next_states: tf.Tensor,\n",
    "    non_final_mask: tf.Tensor,\n",
    "):\n",
    "    with tf.GradientTape() as policy_tape:\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        action_batch = tf.expand_dims(action_batch, 1)\n",
    "        state_action_values = tf.gather_nd(\n",
    "            zombie_policy(state_batch, training=True), action_batch, 1\n",
    "        )\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = tf.scatter_nd(\n",
    "            tf.expand_dims(non_final_mask, 1),\n",
    "            tf.reduce_max(zombie_target(non_final_next_states, training=False), 1),\n",
    "            tf.constant([BATCH_SIZE]),\n",
    "        )\n",
    "        \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "        print(\"state\",state_action_values.shape)\n",
    "        print(\"expected\",expected_state_action_values.shape)\n",
    "        tf.expand_dims(state_action_values, )\n",
    "\n",
    "        # compute loss (mean squared error)\n",
    "        assert state_action_values.shape == expected_state_action_values.shape\n",
    "        _loss = loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    policy_gradient = policy_tape.gradient(_loss, zombie_policy.trainable_variables)\n",
    "\n",
    "    # apply gradient\n",
    "    optimizer.apply_gradients(zip(policy_gradient, zombie_policy.trainable_variables))\n",
    "\n",
    "    # return the loss\n",
    "    return _loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_timesteps, max_timesteps=200, render=False, logdir=\"\", run_name=\"\"):\n",
    "    env = ZombieEnvironment(max_timesteps, logdir=logdir, run_name=run_name, have_enemy_player=True)\n",
    "    if render:\n",
    "        env.init_render()\n",
    "\n",
    "    while env.total_timesteps < num_timesteps:\n",
    "        # Initialize the environment and state\n",
    "        prev_obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_zombie_action(\n",
    "                tf.constant([prev_obs]), env.total_timesteps, env.writer\n",
    "            )\n",
    "            action = action.numpy()[0]  # \"flatten\" the tensor and take the item\n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            # reward = tf.constant([reward])\n",
    "\n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = new_obs\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(prev_obs, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            prev_obs = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "                # detailed explanation). This converts batch-array of Transitions\n",
    "                # to Transition of batch-arrays.\n",
    "                batch = Transition(*zip(*memory.sample(BATCH_SIZE)))\n",
    "\n",
    "                # compute the states that aren't terminal states\n",
    "                non_final_mask = tf.constant(\n",
    "                    tuple(\n",
    "                        idx\n",
    "                        for state, idx in zip(\n",
    "                            batch.next_state, range(len(batch.next_state))\n",
    "                        )\n",
    "                        if state is not None\n",
    "                    ),\n",
    "                )\n",
    "                non_final_next_states = tf.cast(\n",
    "                    tuple(state for state in batch.next_state if state is not None),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "\n",
    "                loss = train_on_batch(\n",
    "                    tf.cast(batch.state, dtype=tf.float32),\n",
    "                    tf.cast(batch.action, dtype=tf.int32),\n",
    "                    tf.cast(batch.reward, dtype=tf.float32),\n",
    "                    non_final_next_states,\n",
    "                    non_final_mask,\n",
    "                )\n",
    "                with env.writer.as_default():\n",
    "                    tf.summary.scalar(\"train/loss\", float(loss.numpy().item()), step=env.total_timesteps)\n",
    "\n",
    "        env.write_run_metrics()\n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if env.total_timesteps % TARGET_UPDATE == 0:\n",
    "            zombie_policy.save_weights(\"zombie_policy_weights\")\n",
    "            zombie_target.load_weights(\"./zombie_policy_weights\")\n",
    "    # env.close()\n",
    "    zombie_policy.save_weights(\"zombie_policy_weights\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a569b528fb1110d0d7d552dfd5bf7c0920d164754c3ee6d9fc5930b2e92fc65e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
