{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import scipy.signal\n",
    "import time\n",
    "from collections import namedtuple, Counter\n",
    "from queue import deque\n",
    "import random\n",
    "import math\n",
    "from typing import List\n",
    "from tqdm import tqdm  # used for progress meters\n",
    "from time import sleep\n",
    "\n",
    "sys.path.append(\"./\")  # make sure that it is able to import Board\n",
    "\n",
    "from Board import Board\n",
    "from constants import *\n",
    "from Player import ZombiePlayer, GovernmentPlayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"GPU\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 4000\n",
    "epochs = 30\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (256, 256)\n",
    "\n",
    "# True if you want to render the environment\n",
    "render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GovernmentEnvironment:\n",
    "    ACTION_MAPPINGS = {\n",
    "        0: \"moveUp\",\n",
    "        1: \"moveDown\",\n",
    "        2: \"moveLeft\",\n",
    "        3: \"moveRight\",\n",
    "        4: \"wallUp\",\n",
    "        5: \"wallDown\",\n",
    "        6: \"wallLeft\",\n",
    "        7: \"wallRight\",\n",
    "        8: \"vaccinate\",\n",
    "        9: \"cureUp\",\n",
    "        10: \"cureDown\",\n",
    "        11: \"cureLeft\",\n",
    "        12: \"cureRight\",\n",
    "    }\n",
    "    ACTION_NAMES = [\n",
    "        \"moveUp\",\n",
    "        \"moveDown\",\n",
    "        \"moveLeft\",\n",
    "        \"moveRight\",\n",
    "        \"wallUp\",\n",
    "        \"wallDown\",\n",
    "        \"wallLeft\",\n",
    "        \"wallRight\",\n",
    "        \"vaccinate\",\n",
    "        \"cureUp\",\n",
    "        \"cureDown\",\n",
    "        \"cureLeft\",\n",
    "        \"cureRight\",\n",
    "    ]\n",
    "    ACTION_SPACE = len(ACTION_MAPPINGS.keys())\n",
    "    SIZE = (6, 6)\n",
    "    OBSERVATION_SPACE = 40\n",
    "\n",
    "    def __init__(self, max_timesteps: int = 300, logdir: str = \"\", run_name=\"\") -> None:\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.reset()\n",
    "        self.total_timesteps = 0\n",
    "        self.total_invalid_moves = 0\n",
    "        self.writer = None\n",
    "        if logdir != \"\" and run_name != \"\":\n",
    "            self.writer = tf.summary.create_file_writer(f\"{logdir}/{run_name}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = Board(GovernmentEnvironment.SIZE, \"Government\")\n",
    "        num_people = random.randint(7, 12)\n",
    "        self.board.populate(num_people=num_people, num_zombies=num_people - 1)\n",
    "        self.enemyPlayer = ZombiePlayer()\n",
    "        self.done = False\n",
    "\n",
    "        # coordinates of the first Government player\n",
    "        self.agentPosition = self.board.indexOf(False)\n",
    "        if self.agentPosition == -1:\n",
    "            self.reset()\n",
    "\n",
    "        # useful for metrics\n",
    "        self.max_number_of_government = 1\n",
    "        self.episode_invalid_actions = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_timesteps = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def get_invalid_action_mask(self):\n",
    "        action_mask = [True for name in GovernmentEnvironment.ACTION_NAMES]\n",
    "        clone = self.board.clone(self.board.States, self.board.player_role)\n",
    "        coord = self.board.toCoord(self.agentPosition)\n",
    "        for idx in range(len(GovernmentEnvironment.ACTION_NAMES)):\n",
    "            action_name = GovernmentEnvironment.ACTION_NAMES[idx]\n",
    "\n",
    "            if \"move\" in action_name:\n",
    "                valid, new_pos = clone.actionToFunction[action_name](coord)\n",
    "            elif \"vaccinate\" in action_name:\n",
    "                valid, _ = clone.actionToFunction[action_name](coord)\n",
    "            elif \"cure\" in action_name:\n",
    "                dest_coord = list(coord)\n",
    "                if action_name == \"cureUp\":\n",
    "                    dest_coord[1] -= 1\n",
    "                elif action_name == \"cureDown\":\n",
    "                    dest_coord[1] += 1\n",
    "                elif action_name == \"cureRight\":\n",
    "                    dest_coord[0] += 1\n",
    "                else:\n",
    "                    dest_coord[0] -= 1\n",
    "                valid, _ = clone.actionToFunction[\"cure\"](dest_coord)\n",
    "            else:  # wall variation\n",
    "                dest_coord = list(coord)\n",
    "                if action_name == \"wallUp\":\n",
    "                    dest_coord[1] -= 1\n",
    "                elif action_name == \"wallDown\":\n",
    "                    dest_coord[1] += 1\n",
    "                elif action_name == \"wallRight\":\n",
    "                    dest_coord[0] += 1\n",
    "                else:\n",
    "                    dest_coord[0] -= 1\n",
    "                valid, _ = clone.actionToFunction[\"wall\"](dest_coord)\n",
    "\n",
    "            if valid:\n",
    "                # re-clone the board\n",
    "                clone = self.board.clone(self.board.States, self.board.player_role)\n",
    "            else:\n",
    "                action_mask[idx] = False\n",
    "\n",
    "        return action_mask\n",
    "\n",
    "    def step(self, action: int):\n",
    "        if self.board.States[self.agentPosition].person is None:\n",
    "            print(\"Lost Person before\")\n",
    "            print(\"agent position is\", self.agentPosition)\n",
    "            print(\"obs is\", self._get_obs())\n",
    "\n",
    "        action_name = GovernmentEnvironment.ACTION_MAPPINGS[action]\n",
    "        # print(\"Before: \", end = str(self.agentPosition))\n",
    "        # print()\n",
    "        # print(action_name)\n",
    "        if \"move\" in action_name:\n",
    "            # print(self.board.get_board())\n",
    "            valid, new_pos = self.board.actionToFunction[action_name](\n",
    "                self.board.toCoord(self.agentPosition)\n",
    "            )\n",
    "            if valid:\n",
    "                # print(self.board.get_board())\n",
    "                # print(self.agentPosition)\n",
    "                self.agentPosition = new_pos\n",
    "                # print(\"After: \", end = str(self.agentPosition))\n",
    "                # print()\n",
    "        elif \"vaccinate\" in action_name:\n",
    "            valid, _ = self.board.actionToFunction[action_name](\n",
    "                self.board.toCoord(self.agentPosition)\n",
    "            )\n",
    "        elif \"cure\" in action_name:\n",
    "            dest_coord = list(self.board.toCoord(self.agentPosition))\n",
    "            if action_name == \"cureUp\":\n",
    "                dest_coord[1] -= 1\n",
    "            elif action_name == \"cureDown\":\n",
    "                dest_coord[1] += 1\n",
    "            elif action_name == \"cureRight\":\n",
    "                dest_coord[0] += 1\n",
    "            else:\n",
    "                dest_coord[0] -= 1\n",
    "            valid, _ = self.board.actionToFunction[\"cure\"](dest_coord)\n",
    "        else:  # wall variation\n",
    "            dest_coord = list(self.board.toCoord(self.agentPosition))\n",
    "            if action_name == \"wallUp\":\n",
    "                dest_coord[1] -= 1\n",
    "            elif action_name == \"wallDown\":\n",
    "                dest_coord[1] += 1\n",
    "            elif action_name == \"wallRight\":\n",
    "                dest_coord[0] += 1\n",
    "            else:\n",
    "                dest_coord[0] -= 1\n",
    "            valid, _ = self.board.actionToFunction[\"wall\"](dest_coord)\n",
    "\n",
    "        won = None\n",
    "        # do the opposing player's action if the action was valid.\n",
    "        if valid:\n",
    "            _action, coord = self.enemyPlayer.get_move(self.board)\n",
    "            if not _action:\n",
    "                self.done = True\n",
    "                won = True\n",
    "            else:\n",
    "                self.board.actionToFunction[_action](coord)\n",
    "            self.board.update()\n",
    "\n",
    "        # see if the game is over\n",
    "        # print(self.agentPosition)\n",
    "        # print(self.board.get_board())\n",
    "        # print(self._get_obs())\n",
    "        if self.board.States[self.agentPosition].person is None:\n",
    "            print(\"Lost Person\")\n",
    "            print(\"agent position is\", self.agentPosition)\n",
    "            print(\"obs is\", self._get_obs())\n",
    "\n",
    "        if self.board.States[self.agentPosition].person.isZombie:  # person was bitten\n",
    "            self.done = True\n",
    "            won = False\n",
    "        if not self.board.is_move_possible_at(self.agentPosition):  # no move possible\n",
    "            self.done = True\n",
    "        if self.episode_timesteps > self.max_timesteps:\n",
    "            self.done = True\n",
    "\n",
    "        # get obs, reward, done, info\n",
    "        obs, reward, done, info = (\n",
    "            self._get_obs(),\n",
    "            self._get_reward(action_name, valid, won),\n",
    "            self._get_done(),\n",
    "            self._get_info(),\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.episode_reward += reward\n",
    "        if not valid:\n",
    "            self.episode_invalid_actions += 1\n",
    "            self.total_invalid_moves += 1\n",
    "        self.episode_timesteps += 1\n",
    "        self.max_number_of_government = max(\n",
    "            self.board.num_people(), self.max_number_of_government\n",
    "        )\n",
    "        self.total_timesteps += 1\n",
    "\n",
    "        # write the metrics\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"train/invalid_action_rate\",\n",
    "                    self.total_invalid_moves / self.total_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\"train/cur_reward\", reward, step=self.total_timesteps)\n",
    "\n",
    "        # return the obs, reward, done, info\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_done(self):\n",
    "        return self.done\n",
    "\n",
    "    def _get_reward(self, action_name: str, was_valid: bool, won: bool):\n",
    "        \"\"\"\n",
    "        Gonna try to return reward between [-1, 1]\n",
    "        This fits w/i tanh and sigmoid ranges\n",
    "        \"\"\"\n",
    "        if not was_valid:\n",
    "            return -1\n",
    "        if won is True:\n",
    "            return 1\n",
    "        if won is False:\n",
    "            return -0.5\n",
    "        if \"vaccinate\" in action_name:\n",
    "            return 0.3\n",
    "        if \"cure\" in action_name:\n",
    "            return 0.7\n",
    "        return -0.01  # this is the case where it was move\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Is based off the assumption that 5 is not in the returned board.\n",
    "        Uses 5 as the key for current position.\n",
    "        \"\"\"\n",
    "        AGENT_POSITION_CONSTANT = 5\n",
    "        ret = self.board.get_board()\n",
    "        ret[self.agentPosition] = AGENT_POSITION_CONSTANT\n",
    "\n",
    "        # add resources and prices\n",
    "        ret.append(self.board.resources.resources)\n",
    "        ret.append(self.board.resources.costs[\"cure\"])\n",
    "        ret.append(self.board.resources.costs[\"vaccinate\"])\n",
    "        ret.append(self.board.resources.costs[\"wall\"])\n",
    "\n",
    "        return np.array(ret, dtype=np.float32)\n",
    "\n",
    "    def render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.run(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def init_render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.initScreen(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        import pygame\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "    def write_run_metrics(self):\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/num_invalid_actions_per_ep\",\n",
    "                    self.episode_invalid_actions,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_length\",\n",
    "                    self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_total_reward\",\n",
    "                    self.episode_reward,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/mean_reward\",\n",
    "                    self.episode_reward / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/percent_invalid_per_ep\",\n",
    "                    self.episode_invalid_actions / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = GovernmentEnvironment.ACTION_SPACE\n",
    "OBS_SPACE = GovernmentEnvironment.OBSERVATION_SPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "def apply_invalid_mask(logits, env:GovernmentEnvironment):\n",
    "    # pass in logits; this would be before doing logprobabilities\n",
    "    # applies an invalid action mask\n",
    "    action_mask = tf.constant([env.get_invalid_action_mask()], dtype=tf.bool)\n",
    "    invalid_values = tf.constant([[tf.float32.min] * NUM_ACTIONS], dtype=tf.float32)\n",
    "    \n",
    "    assert invalid_values.shape == logits.shape \n",
    "    logits = tf.where(action_mask, logits, invalid_values)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    logits = apply_invalid_mask(logits, env)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GovernmentEnvironment(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "temp_obs = env.reset()\n",
    "observation_dimensions = temp_obs.shape[0]\n",
    "num_actions = env.ACTION_SPACE\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, None)\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "value = tf.squeeze(\n",
    "    mlp(observation_input, list(hidden_sizes) + [1], tf.tanh, None), axis=1\n",
    ")\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure logits work\n",
    "for i in range(100):\n",
    "    logits, action = sample_action(np.reshape(observation, (1, 40)))\n",
    "    action = action.numpy()[0]\n",
    "    if action == 12:\n",
    "        print(\"12\")\n",
    "    if action == 13:\n",
    "        print(\"13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20e220032b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#value_optimizer_weights = np.load(\"gov_value_optimizer_v2.npy\")\n",
    "#value_optimizer.set_weights(value_optimizer_weights)\n",
    "\n",
    "#policy_optimizer_weights = np.load(\"gov_policy_optimizer_v2.npy\")\n",
    "#policy_optimizer.set_weights(policy_optimizer_weights)\n",
    "\n",
    "actor.load_weights(\"gov_actor_v2_weights\")\n",
    "critic.load_weights(\"gov_critic_v2_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    global observation, episode_return, episode_length\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "        sum_return = 0\n",
    "        sum_length = 0\n",
    "        num_episodes = 0\n",
    "\n",
    "        # Iterate over the steps of each epoch\n",
    "        for t in range(steps_per_epoch):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Get the logits, action, and take one step in the environment\n",
    "            observation = observation.reshape(1, -1)\n",
    "            logits, action = sample_action(observation)\n",
    "            observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get the value and log-probability of the action\n",
    "            value_t = critic(observation)\n",
    "            logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "            # Store obs, act, rew, v_t, logp_pi_t\n",
    "            buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "            # Update the observation\n",
    "            observation = observation_new\n",
    "\n",
    "            # Finish trajectory if reached to a terminal state\n",
    "            terminal = done\n",
    "            if terminal or (t == steps_per_epoch - 1):\n",
    "                last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "                buffer.finish_trajectory(last_value)\n",
    "                sum_return += episode_return\n",
    "                sum_length += episode_length\n",
    "                num_episodes += 1\n",
    "                observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "        # Get values from the buffer\n",
    "        (\n",
    "            observation_buffer,\n",
    "            action_buffer,\n",
    "            advantage_buffer,\n",
    "            return_buffer,\n",
    "            logprobability_buffer,\n",
    "        ) = buffer.get()\n",
    "\n",
    "        # Update the policy and implement early stopping using KL divergence\n",
    "        for _ in range(train_policy_iterations):\n",
    "            kl = train_policy(\n",
    "                observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "            )\n",
    "            if kl > 1.5 * target_kl:\n",
    "                # Early Stopping\n",
    "                break\n",
    "\n",
    "        # Update the value function\n",
    "        for _ in range(train_value_iterations):\n",
    "            train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "        # Print mean return and length for each epoch\n",
    "        print(\n",
    "            f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:34<05:10, 34.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 1. Mean Return: -0.023950892857142834. Mean Length: 8.928571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:07<04:29, 33.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 2. Mean Return: -0.0300447427293065. Mean Length: 8.94854586129754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:42<04:00, 34.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 3. Mean Return: -0.0016009280742458765. Mean Length: 9.280742459396752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:19<03:31, 35.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 4. Mean Return: -0.060831509846827114. Mean Length: 8.752735229759299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:54<02:55, 35.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 5. Mean Return: -0.01970319634703198. Mean Length: 9.132420091324201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:29<02:19, 34.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 6. Mean Return: -0.04129670329670328. Mean Length: 8.791208791208792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:04<01:45, 35.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 7. Mean Return: -0.0021064814814814826. Mean Length: 9.25925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:39<01:09, 35.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 8. Mean Return: -0.11072072072072085. Mean Length: 9.00900900900901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:14<00:35, 35.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 9. Mean Return: 0.06234299516908215. Mean Length: 9.66183574879227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:49<00:00, 34.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 10. Mean Return: -0.03091139240506324. Mean Length: 10.126582278481013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "with tf.device(DEVICE):\n",
    "    train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GovernmentEnvironment(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    o, r, d, _ = env.step(random.randint(0, 12))\n",
    "    if d:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_model(render: bool = False, max_timesteps=200):\n",
    "    env = GovernmentEnvironment(max_timesteps)\n",
    "    done = False\n",
    "    if render:\n",
    "        env.init_render()\n",
    "    obs = env.reset()\n",
    "    actions = []\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "            sleep(0.1)\n",
    "        # logits, action = sample_action(np.reshape(obs, (1, obs.shape[0])))\n",
    "        logits = actor(np.reshape(obs, (1, obs.shape[0])))\n",
    "        logits = apply_invalid_mask(logits, env)\n",
    "        action = tf.argmax(tf.squeeze(logits)).numpy()\n",
    "\n",
    "        print(f\"took action {action} which is {env.ACTION_MAPPINGS[action]}\")\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "    if render:\n",
    "        env.render()\n",
    "        sleep(0.1)\n",
    "        env.close()\n",
    "    counter = Counter(actions)\n",
    "    print(counter.most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took action 1 which is moveDown\n",
      "[(1, 1)]\n"
     ]
    }
   ],
   "source": [
    "watch_model(render=True, max_timesteps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.save_weights(\"gov_actor_v2_weights\")\n",
    "critic.save_weights(\"gov_critic_v2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a569b528fb1110d0d7d552dfd5bf7c0920d164754c3ee6d9fc5930b2e92fc65e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
