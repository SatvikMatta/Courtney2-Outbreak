{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGAI models (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based off of the pytorch tutorial [here](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html). It is intended to both create and train models for Courtney2-Outbreak. View on Colab [here](https://drive.google.com/file/d/1paNMxYQ6wVQ8c5bKJf-u3rHDcqgpKOB0/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "import keras.models as models\n",
    "import keras\n",
    "from collections import namedtuple, Counter\n",
    "from queue import deque\n",
    "import random\n",
    "import math\n",
    "from typing import List\n",
    "from tqdm import tqdm  # used for progress meters\n",
    "\n",
    "sys.path.append(\"./\")  # make sure that it is able to import Board\n",
    "\n",
    "from Board import Board\n",
    "from constants import *\n",
    "from Player import ZombiePlayer, GovernmentPlayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"GPU\"\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "devices = tf.config.list_physical_devices(DEVICE)\n",
    "print(devices)\n",
    "if DEVICE == \"GPU\":\n",
    "    tf.config.experimental.set_memory_growth(devices[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZombieEnvironment:\n",
    "    ACTION_SPACE = tuple(range(8))\n",
    "    ACTION_MAPPINGS = {\n",
    "        0: \"moveUp\",\n",
    "        1: \"moveDown\",\n",
    "        2: \"moveLeft\",\n",
    "        3: \"moveRight\",\n",
    "        4: \"biteUp\",\n",
    "        5: \"biteDown\",\n",
    "        6: \"biteLeft\",\n",
    "        7: \"biteRight\",\n",
    "    }\n",
    "    SIZE = (6, 6)\n",
    "\n",
    "    def __init__(self, max_timesteps: int = 300, logdir: str = \"\", run_name=\"\") -> None:\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.reset()\n",
    "        self.total_timesteps = 0\n",
    "        self.total_invalid_moves = 0\n",
    "        self.writer = None\n",
    "        if logdir != \"\" and run_name != \"\":\n",
    "            self.writer = tf.summary.create_file_writer(f\"{logdir}/{run_name}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = Board(ZombieEnvironment.SIZE, \"Zombie\")\n",
    "        self.board.populate(num_zombies=1)\n",
    "        self.enemyPlayer = GovernmentPlayer()\n",
    "        self.done = False\n",
    "\n",
    "        # coordinates of the first zombie\n",
    "        self.agentPosition = self.board.indexOf(True)\n",
    "\n",
    "        # useful for metrics\n",
    "        self.max_number_of_zombies = 1\n",
    "        self.episode_invalid_actions = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_timesteps = 0\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        action_name = ZombieEnvironment.ACTION_MAPPINGS[action]\n",
    "        if \"move\" in action_name:\n",
    "            valid, new_pos = self.board.actionToFunction[action_name](\n",
    "                self.board.toCoord(self.agentPosition)\n",
    "            )\n",
    "            if valid:\n",
    "                self.agentPosition = new_pos\n",
    "        else:  # bite variation\n",
    "            dest_coord = list(self.board.toCoord(self.agentPosition))\n",
    "            if action_name == \"biteUp\":\n",
    "                dest_coord[1] -= 1\n",
    "            elif action_name == \"biteDown\":\n",
    "                dest_coord[1] += 1\n",
    "            elif action_name == \"biteRight\":\n",
    "                dest_coord[0] += 1\n",
    "            else:\n",
    "                dest_coord[0] -= 1\n",
    "            valid, _ = self.board.actionToFunction[\"bite\"](dest_coord)\n",
    "\n",
    "        won = None\n",
    "        # do the opposing player's action if the action was valid.\n",
    "        if valid:\n",
    "            _action, coord = self.enemyPlayer.get_move(self.board)\n",
    "            if not _action:\n",
    "                self.done = True\n",
    "                won = True\n",
    "            else:\n",
    "                self.board.actionToFunction[_action](coord)\n",
    "            self.board.update()\n",
    "\n",
    "        # see if the game is over\n",
    "        if not self.board.States[\n",
    "            self.agentPosition\n",
    "        ].person.isZombie:  # zombie was cured\n",
    "            self.done = True\n",
    "            won = False\n",
    "        if not self.board.is_move_possible_at(self.agentPosition):  # no move possible\n",
    "            self.done = True\n",
    "        if self.episode_timesteps > self.max_timesteps:\n",
    "            self.done = True\n",
    "\n",
    "        # get obs, reward, done, info\n",
    "        obs, reward, done, info = (\n",
    "            self._get_obs(),\n",
    "            self._get_reward(action_name, valid, won),\n",
    "            self._get_done(),\n",
    "            self._get_info(),\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.episode_reward += reward\n",
    "        if not valid:\n",
    "            self.episode_invalid_actions += 1\n",
    "            self.total_invalid_moves += 1\n",
    "        self.episode_timesteps += 1\n",
    "        self.max_number_of_zombies = max(\n",
    "            self.board.num_zombies(), self.max_number_of_zombies\n",
    "        )\n",
    "        self.total_timesteps += 1\n",
    "\n",
    "        # write the metrics\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"train/invalid_action_rate\",\n",
    "                    self.total_invalid_moves / self.total_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\"train/cur_reward\", reward, step=self.total_timesteps)\n",
    "\n",
    "        # return the obs, reward, done, info\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_done(self):\n",
    "        return self.done\n",
    "\n",
    "    def _get_reward(self, action_name: str, was_valid: bool, won: bool):\n",
    "        \"\"\"\n",
    "        Gonna try to return reward between [-1, 1]\n",
    "        This fits w/i tanh and sigmoid ranges\n",
    "        \"\"\"\n",
    "        if not was_valid:\n",
    "            return -1\n",
    "        if won is True:\n",
    "            return 1\n",
    "        if won is False:\n",
    "            return -0.5\n",
    "        if \"bite\" in action_name:\n",
    "            return 0.3\n",
    "        return 0.01  # this is the case where it was move\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Is based off the assumption that 5 is not in the returned board.\n",
    "        Uses 5 as the key for current position.\n",
    "        \"\"\"\n",
    "        AGENT_POSITION_CONSTANT = 5\n",
    "        ret = self.board.get_board()\n",
    "        ret[self.agentPosition] = AGENT_POSITION_CONSTANT\n",
    "        \n",
    "        # normalize observation to be be centered at 0\n",
    "        ret = np.array(ret, dtype=np.float32)\n",
    "        ret /= np.float32(AGENT_POSITION_CONSTANT)\n",
    "        ret -= np.float32(0.5)\n",
    "        return ret\n",
    "\n",
    "    def render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.run(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def init_render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.initScreen(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        import pygame\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "    def write_run_metrics(self):\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/num_invalid_actions_per_ep\",\n",
    "                    self.episode_invalid_actions,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_length\",\n",
    "                    self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_total_reward\",\n",
    "                    self.episode_reward,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/mean_reward\",\n",
    "                    self.episode_reward / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/percent_invalid_per_ep\",\n",
    "                    self.episode_invalid_actions / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3       ,  0.10000002,  0.10000002,  0.10000002,  0.10000002,\n",
       "        0.5       , -0.3       , -0.3       , -0.5       , -0.3       ,\n",
       "       -0.3       , -0.5       , -0.3       , -0.5       , -0.3       ,\n",
       "       -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "       -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "       -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "       -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "       -0.5       ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to make sure that the observation is what we want.\n",
    "test_env = ZombieEnvironment()\n",
    "test_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZOMBIE_OUTPUT_SIZE = len(ZombieEnvironment.ACTION_SPACE)\n",
    "INPUT_SHAPE = (ROWS * COLUMNS,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zombie_model():\n",
    "    \"\"\"\n",
    "    makes the model that will be used for zombies\n",
    "    The output of the model will be the predicted q value\n",
    "    for being in a certain state.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(INPUT_SHAPE))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(36 * 2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 4))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 8))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 16))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 32))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE * 16))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE * 8))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE * 4))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE * 2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE, activation='tanh'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    zombie_policy = make_zombie_model()\n",
    "    zombie_target = make_zombie_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 36)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 36)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 72)                2664      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 72)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 144)               10512     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 144)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 288)               41760     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 288)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 576)               166464    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 576)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1152)              664704    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               147584    \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,044,688\n",
      "Trainable params: 1,044,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(zombie_policy.input_shape)\n",
    "zombie_policy.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "tf.Tensor(\n",
      "[[-0.03697127 -0.01303399  0.00346363  0.06687117  0.04757929 -0.09484172\n",
      "  -0.01469022  0.02574408]], shape=(1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# make sure the output is correct shape and is between [-1, 1]\n",
    "with tf.device(DEVICE):\n",
    "    temp = zombie_policy(tf.random.normal((1, 36)), training=False)\n",
    "print(temp.shape)\n",
    "print(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zombie_policy.load_weights(\"zombie_policy_weights\")\n",
    "# zombie_target.load_weights(\"zombie_policy_weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this acts as a class; useful in the training\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    optimizer = keras.optimizers.Adam(0.004)\n",
    "    loss = keras.losses.Huber()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.999\n",
    "EPSILON_MAX = 0.9  # exploration rate maximum\n",
    "EPSILON_MIN = 0.05  # exploration rate minimum\n",
    "EPS_DECAY = 1000  # decay rate, in steps\n",
    "TARGET_UPDATE = 10  # how many episodes before the target is updated\n",
    "\n",
    "BUFFER_CAPACITY = 10000\n",
    "memory = ReplayMemory(BUFFER_CAPACITY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_zombie_action(state, steps_done: int = -1, writer=None):\n",
    "    \"\"\"\n",
    "    If no steps are provided, assuming not going to do\n",
    "    random exploration\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    eps_threshold = 0\n",
    "    if steps_done != -1:\n",
    "        eps_threshold = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN) * math.exp(\n",
    "            -1.0 * steps_done / EPS_DECAY\n",
    "        )\n",
    "    if writer is not None:\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('exploration rate', eps_threshold, step=steps_done)\n",
    "    if sample > eps_threshold:\n",
    "        # Pick the action with the largest expected reward.\n",
    "        temp = zombie_policy(state, training=False)\n",
    "        numpy = temp.numpy().flatten()\n",
    "        return tf.constant([tuple(numpy).index(max(numpy))], dtype=tf.int32)\n",
    "    else:\n",
    "        return tf.constant([random.randrange(ZOMBIE_OUTPUT_SIZE)], dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_on_batch(\n",
    "    state_batch: tf.Tensor,\n",
    "    action_batch: tf.Tensor,\n",
    "    reward_batch: tf.Tensor,\n",
    "    non_final_next_states: tf.Tensor,\n",
    "    non_final_mask: tf.Tensor,\n",
    "):\n",
    "    with tf.GradientTape() as policy_tape:\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        action_batch = tf.expand_dims(action_batch, 1)\n",
    "        state_action_values = tf.gather_nd(\n",
    "            zombie_policy(state_batch, training=True), action_batch, 1\n",
    "        )\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = tf.scatter_nd(\n",
    "            tf.expand_dims(non_final_mask, 1),\n",
    "            tf.reduce_max(zombie_target(non_final_next_states, training=False), 1),\n",
    "            tf.constant([BATCH_SIZE]),\n",
    "        )\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = tf.squeeze(\n",
    "            (next_state_values * GAMMA) + reward_batch\n",
    "        )\n",
    "\n",
    "        # compute loss (mean squared error)\n",
    "        assert state_action_values.shape == expected_state_action_values.shape\n",
    "        _loss = loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    policy_gradient = policy_tape.gradient(_loss, zombie_policy.trainable_variables)\n",
    "\n",
    "    # apply gradient\n",
    "    optimizer.apply_gradients(zip(policy_gradient, zombie_policy.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, max_timesteps=200, render=False, logdir=\"\", run_name=\"\"):\n",
    "    env = ZombieEnvironment(max_timesteps, logdir, run_name)\n",
    "    if render:\n",
    "        env.init_render()\n",
    "\n",
    "    for episode in tqdm(range(epochs)):\n",
    "        # Initialize the environment and state\n",
    "        prev_obs = env.reset()\n",
    "        done = False\n",
    "        timesteps = 0\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_zombie_action(\n",
    "                tf.constant([prev_obs]), env.total_timesteps, env.writer\n",
    "            )\n",
    "            action = action.numpy()[0]  # \"flatten\" the tensor and take the item\n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            # reward = tf.constant([reward])\n",
    "\n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = new_obs\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(prev_obs, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            prev_obs = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "                # detailed explanation). This converts batch-array of Transitions\n",
    "                # to Transition of batch-arrays.\n",
    "                batch = Transition(*zip(*memory.sample(BATCH_SIZE)))\n",
    "\n",
    "                # compute the states that aren't terminal states\n",
    "                non_final_mask = tf.constant(\n",
    "                    tuple(\n",
    "                        idx\n",
    "                        for state, idx in zip(\n",
    "                            batch.next_state, range(len(batch.next_state))\n",
    "                        )\n",
    "                        if state is not None\n",
    "                    ),\n",
    "                )\n",
    "                non_final_next_states = tf.cast(\n",
    "                    tuple(state for state in batch.next_state if state is not None),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "\n",
    "                train_on_batch(\n",
    "                    tf.cast(batch.state, dtype=tf.float32),\n",
    "                    tf.cast(batch.action, dtype=tf.int32),\n",
    "                    tf.cast(batch.reward, dtype=tf.float32),\n",
    "                    non_final_next_states,\n",
    "                    non_final_mask,\n",
    "                )\n",
    "\n",
    "        env.write_run_metrics()\n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            zombie_policy.save_weights(\"zombie_policy_weights\")\n",
    "            zombie_target.load_weights(\"./zombie_policy_weights\")\n",
    "    # env.close()\n",
    "    zombie_policy.save_weights(\"zombie_policy_weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NUMBER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:53<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    train(100, 100, render=False, logdir=\"zombieEnvironment\", run_name=f\"run{RUN_NUMBER}\")\n",
    "    RUN_NUMBER+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_model(max_timesteps=200):\n",
    "    env = ZombieEnvironment(max_timesteps)\n",
    "    done = False\n",
    "    env.init_render()\n",
    "    obs = env.reset()\n",
    "    actions = []\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = select_zombie_action(tf.constant([obs])).numpy()[0]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "    env.close()\n",
    "    counter = Counter(actions)\n",
    "    print(counter.most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.10.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "[(1, 202)]\n"
     ]
    }
   ],
   "source": [
    "watch_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a569b528fb1110d0d7d552dfd5bf7c0920d164754c3ee6d9fc5930b2e92fc65e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
