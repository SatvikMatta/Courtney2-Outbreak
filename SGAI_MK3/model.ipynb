{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGAI models (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based off of the pytorch tutorial [here](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html). It is intended to both create and train models for Courtney2-Outbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "import keras.models as models\n",
    "import keras\n",
    "from collections import namedtuple\n",
    "from queue import deque\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm  # used for progress meters\n",
    "sys.path.append(\"./\")  # make sure that it is able to import Board\n",
    "\n",
    "from Board import Board\n",
    "from constants import *\n",
    "from Player import ZombiePlayer, GovernmentPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"GPU\"\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "devices = tf.config.list_physical_devices(DEVICE)\n",
    "print(devices)\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZombieEnvironment:\n",
    "    ACTION_SPACE = tuple(range(8))\n",
    "    ACTION_MAPPINGS = {\n",
    "        0: \"moveUp\",\n",
    "        1: \"moveDown\",\n",
    "        2: \"moveLeft\",\n",
    "        3: \"moveRight\",\n",
    "        4: \"biteUp\",\n",
    "        5: \"biteDown\",\n",
    "        6: \"biteLeft\",\n",
    "        7: \"biteRight\",\n",
    "    }\n",
    "    SIZE = (6, 6)\n",
    "\n",
    "    def __init__(self, max_timesteps: int = 300, logdir: str = \"\", run_name=\"\") -> None:\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.reset()\n",
    "        self.total_timesteps = 0\n",
    "        self.total_invalid_moves = 0\n",
    "        self.writer = None\n",
    "        if logdir != \"\" and run_name != \"\":\n",
    "            self.writer = tf.summary.create_file_writer(f\"{logdir}/{run_name}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = Board(ZombieEnvironment.SIZE, \"Zombie\")\n",
    "        self.board.populate(num_zombies=1)\n",
    "        self.enemyPlayer = GovernmentPlayer()\n",
    "        self.done = False\n",
    "\n",
    "        # coordinates of the first zombie\n",
    "        self.agentPosition = self.board.indexOf(True)\n",
    "\n",
    "        # useful for metrics\n",
    "        self.max_number_of_zombies = 1\n",
    "        self.episode_invalid_actions = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_timesteps = 0\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        action_name = ZombieEnvironment.ACTION_MAPPINGS[action]\n",
    "        if \"move\" in action_name:\n",
    "            valid, new_pos = self.board.actionToFunction[action_name](\n",
    "                self.board.toCoord(self.agentPosition)\n",
    "            )\n",
    "            if valid:\n",
    "                self.agentPosition = new_pos\n",
    "        else:  # bite variation\n",
    "            dest_coord = list(self.board.toCoord(self.agentPosition))\n",
    "            if action_name == \"biteUp\":\n",
    "                dest_coord[1] -= 1\n",
    "            elif action_name == \"biteDown\":\n",
    "                dest_coord[1] += 1\n",
    "            elif action_name == \"biteRight\":\n",
    "                dest_coord[0] += 1\n",
    "            else:\n",
    "                dest_coord[0] -= 1\n",
    "            valid, _ = self.board.actionToFunction[\"bite\"](dest_coord)\n",
    "\n",
    "        won = None\n",
    "        # do the opposing player's action if the action was valid.\n",
    "        if valid:\n",
    "            _action, coord = self.enemyPlayer.get_move(self.board)\n",
    "            if not _action:\n",
    "                self.done = True\n",
    "                won = True\n",
    "            else:\n",
    "                self.board.actionToFunction[_action](coord)\n",
    "            self.board.update()\n",
    "\n",
    "        # see if the game is over\n",
    "        if not self.board.States[\n",
    "            self.agentPosition\n",
    "        ].person.isZombie:  # zombie was cured\n",
    "            self.done = True\n",
    "            won = False\n",
    "        if not self.board.is_move_possible_at(self.agentPosition):  # no move possible\n",
    "            self.done = True\n",
    "        if self.episode_timesteps > self.max_timesteps:\n",
    "            self.done = True\n",
    "\n",
    "        # get obs, reward, done, info\n",
    "        obs, reward, done, info = (\n",
    "            self._get_obs(),\n",
    "            self._get_reward(action_name, valid, won),\n",
    "            self._get_done(),\n",
    "            self._get_info(),\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.episode_reward += reward\n",
    "        if not valid:\n",
    "            self.episode_invalid_actions += 1\n",
    "            self.total_invalid_moves += 1\n",
    "        self.episode_timesteps += 1\n",
    "        self.max_number_of_zombies = max(\n",
    "            self.board.num_zombies(), self.max_number_of_zombies\n",
    "        )\n",
    "        self.total_timesteps += 1\n",
    "\n",
    "        # write the metrics\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"train/invalid_action_rate\",\n",
    "                    self.total_invalid_moves / self.total_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\"train/cur_reward\", reward, step=self.total_timesteps)\n",
    "\n",
    "        # return the obs, reward, done, info\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_done(self):\n",
    "        return self.done\n",
    "\n",
    "    def _get_reward(self, action_name: str, was_valid: bool, won: bool):\n",
    "        if not was_valid:\n",
    "            return -100\n",
    "        if won is True:\n",
    "            return 100\n",
    "        if won is False:\n",
    "            return -100\n",
    "        if \"bite\" in action_name:\n",
    "            return 15\n",
    "        return 1  # this is the case where it was move\n",
    "\n",
    "    def _get_obs(self):\n",
    "        ret = self.board.get_board()\n",
    "        ret[self.agentPosition] = 4\n",
    "        return np.array(ret)\n",
    "\n",
    "    def render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.run(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def init_render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.initScreen(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        import pygame\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "    def write_run_metrics(self):\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/num_invalid_actions_per_ep\",\n",
    "                    self.episode_invalid_actions,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_length\", self.episode_timesteps, step=self.total_timesteps\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_total_reward\",\n",
    "                    self.episode_reward,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/mean_reward\",\n",
    "                    self.episode_reward / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/percent_invalid_per_ep\",\n",
    "                    self.episode_invalid_actions / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZOMBIE_OUTPUT_SIZE = len(ZombieEnvironment.ACTION_SPACE)\n",
    "INPUT_SHAPE = (ROWS * COLUMNS,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zombie_model():\n",
    "    \"\"\"\n",
    "    makes the model that will be used for zombies\n",
    "    The output of the model will be the predicted q value\n",
    "    for being in a certain state.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(INPUT_SHAPE))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(64))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    zombie_policy = make_zombie_model()\n",
    "    zombie_target = make_zombie_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 36)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 36)                0         \n",
      "                                                                 \n",
      " sync_batch_normalization (S  (None, 36)               144       \n",
      " yncBatchNormalization)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2368      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,608\n",
      "Trainable params: 19,536\n",
      "Non-trainable params: 72\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(zombie_policy.input_shape)\n",
    "zombie_policy.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "# make sure the output is correct shape\n",
    "with tf.device(DEVICE):\n",
    "    temp = zombie_policy(tf.random.normal((1, 36)), training=False)\n",
    "print(temp.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zombie_policy.load_weights(\"zombie_policy_weights\")\n",
    "#zombie_target.load_weights(\"zombie_policy_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this acts as a class; useful in the training\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    optimizer = keras.optimizers.Adam(0.004)\n",
    "    loss = keras.losses.MeanSquaredError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPSILON = 0.05  # exploration rate\n",
    "TARGET_UPDATE = 5  # how many episodes before the target is updated\n",
    "\n",
    "BUFFER_CAPACITY = 10000\n",
    "memory = ReplayMemory(BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_zombie_action(state):\n",
    "    sample = random.random()\n",
    "    if sample > EPSILON:\n",
    "        # Pick the action with the largest expected reward.\n",
    "        temp = zombie_policy(state, training=False)\n",
    "        numpy = temp.numpy().flatten()\n",
    "        return tf.constant([tuple(numpy).index(max(numpy))])\n",
    "    else:\n",
    "        return tf.constant([random.randrange(ZOMBIE_OUTPUT_SIZE)], dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def train_on_batch(state_batch:tf.Tensor, action_batch:tf.Tensor, reward_batch:tf.Tensor, non_final_next_states:tf.Tensor, non_final_mask:tf.Tensor, ):\n",
    "    with tf.GradientTape() as policy_tape:\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        action_batch = tf.expand_dims(action_batch, 1)\n",
    "        state_action_values = tf.gather_nd(zombie_policy(state_batch, training=True), action_batch, 1)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = tf.scatter_nd(\n",
    "            tf.expand_dims(non_final_mask, 1),\n",
    "            tf.reduce_max(zombie_target(non_final_next_states, training=False), 1),\n",
    "            tf.constant([BATCH_SIZE]),\n",
    "        )\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = tf.squeeze((next_state_values * GAMMA) + reward_batch)\n",
    "\n",
    "        # compute loss (mean squared error)\n",
    "        assert state_action_values.shape == expected_state_action_values.shape\n",
    "        _loss = loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    policy_gradient = policy_tape.gradient(_loss, zombie_policy.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(policy_gradient, zombie_policy.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, max_timesteps=200, logdir=\"\", run_name=\"\"):\n",
    "    env = ZombieEnvironment(max_timesteps, logdir, run_name)\n",
    "    env.init_render()\n",
    "    for episode in tqdm(range(epochs)):\n",
    "        # Initialize the environment and state\n",
    "        prev_obs = env.reset()\n",
    "        done = False\n",
    "        timesteps = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            # Select and perform an action\n",
    "            action = select_zombie_action(tf.constant([prev_obs]))\n",
    "            action = action.numpy()[0]  # \"flatten\" the tensor and take the item\n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            # reward = tf.constant([reward])\n",
    "\n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = new_obs\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(prev_obs, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            prev_obs = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "                # detailed explanation). This converts batch-array of Transitions\n",
    "                # to Transition of batch-arrays.\n",
    "                batch = Transition(*zip(*memory.sample(BATCH_SIZE)))\n",
    "\n",
    "                # compute the states that aren't terminal states\n",
    "                non_final_mask = tf.constant(tuple(idx for state, idx in zip(batch.next_state, range(len(batch.next_state))) if state is not None),)\n",
    "                non_final_next_states = tf.cast(tuple(state for state in batch.next_state if state is not None), dtype=tf.float32)\n",
    "\n",
    "                train_on_batch(\n",
    "                    tf.concat([batch.state], 0),\n",
    "                    tf.concat([batch.action], 0),\n",
    "                    tf.cast([batch.reward], dtype=tf.float32),\n",
    "                    non_final_next_states,\n",
    "                    non_final_mask\n",
    "                )\n",
    "\n",
    "        env.write_run_metrics()\n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            zombie_policy.save_weights(\"zombie_policy_weights\")\n",
    "            # zombie_target.load_state_dict(zombie_policy.state_dict())\n",
    "            zombie_target.load_weights(\"./zombie_policy_weights\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action is (128,)\n",
      "action is (128, 1)\n",
      "before gathering, state is (128, 8)\n",
      "after gathering, state is (128,)\n",
      "expected is (1, 128)\n",
      "expected is now (128,)\n",
      "action is (128,)\n",
      "action is (128, 1)\n",
      "before gathering, state is (128, 8)\n",
      "after gathering, state is (128,)\n",
      "expected is (1, 128)\n",
      "expected is now (128,)\n",
      "action is (128,)\n",
      "action is (128, 1)\n",
      "before gathering, state is (128, 8)\n",
      "after gathering, state is (128,)\n",
      "expected is (1, 128)\n",
      "expected is now (128,)\n",
      "action is (128,)\n",
      "action is (128, 1)\n",
      "before gathering, state is (128, 8)\n",
      "after gathering, state is (128,)\n",
      "expected is (1, 128)\n",
      "expected is now (128,)\n",
      "action is (128,)\n",
      "action is (128, 1)\n",
      "before gathering, state is (128, 8)\n",
      "after gathering, state is (128,)\n",
      "expected is (1, 128)\n",
      "expected is now (128,)\n",
      "action is (128,)\n",
      "action is (128, 1)\n",
      "before gathering, state is (128, 8)\n",
      "after gathering, state is (128,)\n",
      "expected is (1, 128)\n",
      "expected is now (128,)\n",
      "action is (128,)\n",
      "action is (128, 1)\n",
      "before gathering, state is (128, 8)\n",
      "after gathering, state is (128,)\n",
      "expected is (1, 128)\n",
      "expected is now (128,)\n",
      "action is (128,)\n",
      "action is (128, 1)\n",
      "before gathering, state is (128, 8)\n",
      "after gathering, state is (128,)\n",
      "expected is (1, 128)\n",
      "expected is now (128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action is (128,)\n",
      "action is (128, 1)\n",
      "before gathering, state is (128, 8)\n",
      "after gathering, state is (128,)\n",
      "expected is (1, 128)\n",
      "expected is now (128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Eliot\\Documents\\GitHub\\Courtney2-Outbreak\\SGAI_MK3\\model.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(DEVICE):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=1'>2</a>\u001b[0m     train(\u001b[39m50\u001b[39;49m, \u001b[39m300\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mzombieEnvironment\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrun1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Eliot\\Documents\\GitHub\\Courtney2-Outbreak\\SGAI_MK3\\model.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, max_timesteps, logdir, run_name)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=9'>10</a>\u001b[0m env\u001b[39m.\u001b[39mrender()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=10'>11</a>\u001b[0m \u001b[39m# Select and perform an action\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=11'>12</a>\u001b[0m action \u001b[39m=\u001b[39m select_zombie_action(tf\u001b[39m.\u001b[39;49mconstant([prev_obs]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=12'>13</a>\u001b[0m action \u001b[39m=\u001b[39m action\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]  \u001b[39m# \"flatten\" the tensor and take the item\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=13'>14</a>\u001b[0m new_obs, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[1;32mc:\\Users\\Eliot\\Documents\\GitHub\\Courtney2-Outbreak\\SGAI_MK3\\model.ipynb Cell 26\u001b[0m in \u001b[0;36mselect_zombie_action\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=1'>2</a>\u001b[0m sample \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandom()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m sample \u001b[39m>\u001b[39m EPSILON:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=3'>4</a>\u001b[0m     \u001b[39m# Pick the action with the largest expected reward.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=4'>5</a>\u001b[0m     temp \u001b[39m=\u001b[39m zombie_policy(state, training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=5'>6</a>\u001b[0m     numpy \u001b[39m=\u001b[39m temp\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mconstant([\u001b[39mtuple\u001b[39m(numpy)\u001b[39m.\u001b[39mindex(\u001b[39mmax\u001b[39m(numpy))])\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:490\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    488\u001b[0m   layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 490\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\sequential.py:374\u001b[0m, in \u001b[0;36mSequential.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    372\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m    373\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_graph_network(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)\n\u001b[1;32m--> 374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(Sequential, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcall(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[0;32m    376\u001b[0m outputs \u001b[39m=\u001b[39m inputs  \u001b[39m# handle the corner case where self.layers is empty\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m    378\u001b[0m   \u001b[39m# During each iteration, `inputs` are the inputs to `layer`, and `outputs`\u001b[39;00m\n\u001b[0;32m    379\u001b[0m   \u001b[39m# are the outputs of `layer` applied to `inputs`. At the end of each\u001b[39;00m\n\u001b[0;32m    380\u001b[0m   \u001b[39m# iteration `inputs` is set to `outputs` to prepare for the next layer.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:458\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    441\u001b[0m   \u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \n\u001b[0;32m    443\u001b[0m \u001b[39m  In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(\n\u001b[0;32m    459\u001b[0m       inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:596\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    593\u001b[0m   \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 596\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    598\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:893\u001b[0m, in \u001b[0;36mBatchNormalizationBase.call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39mif\u001b[39;00m scale \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    892\u001b[0m   scale \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(scale, inputs\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> 893\u001b[0m outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mbatch_normalization(inputs, _broadcast(mean),\n\u001b[0;32m    894\u001b[0m                                     _broadcast(variance), offset, scale,\n\u001b[0;32m    895\u001b[0m                                     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon)\n\u001b[0;32m    896\u001b[0m \u001b[39mif\u001b[39;00m inputs_dtype \u001b[39min\u001b[39;00m (tf\u001b[39m.\u001b[39mfloat16, tf\u001b[39m.\u001b[39mbfloat16):\n\u001b[0;32m    897\u001b[0m   outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(outputs, inputs_dtype)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1590\u001b[0m, in \u001b[0;36mbatch_normalization\u001b[1;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[0;32m   1587\u001b[0m   inv \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m scale\n\u001b[0;32m   1588\u001b[0m \u001b[39m# Note: tensorflow/contrib/quantize/python/fold_batch_norms.py depends on\u001b[39;00m\n\u001b[0;32m   1589\u001b[0m \u001b[39m# the precise order of ops that are generated by the expression below.\u001b[39;00m\n\u001b[1;32m-> 1590\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39;49m math_ops\u001b[39m.\u001b[39;49mcast(inv, x\u001b[39m.\u001b[39;49mdtype) \u001b[39m+\u001b[39m math_ops\u001b[39m.\u001b[39mcast(\n\u001b[0;32m   1591\u001b[0m     offset \u001b[39m-\u001b[39m mean \u001b[39m*\u001b[39m inv \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mmean \u001b[39m*\u001b[39m inv, x\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1406\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1402\u001b[0m   \u001b[39m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1403\u001b[0m   \u001b[39m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m   \u001b[39m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m   x, y \u001b[39m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[1;32m-> 1406\u001b[0m   \u001b[39mreturn\u001b[39;00m func(x, y, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   1407\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1408\u001b[0m   \u001b[39m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m   \u001b[39m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1412\u001b[0m   \u001b[39m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m   \u001b[39m# informative.\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mtype\u001b[39m(y), \u001b[39m\"\u001b[39m\u001b[39m__r\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m op_name):\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1766\u001b[0m, in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m   \u001b[39mreturn\u001b[39;00m sparse_tensor\u001b[39m.\u001b[39mSparseTensor(y\u001b[39m.\u001b[39mindices, new_vals, y\u001b[39m.\u001b[39mdense_shape)\n\u001b[0;32m   1765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1766\u001b[0m   \u001b[39mreturn\u001b[39;00m multiply(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:529\u001b[0m, in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmath.multiply\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmultiply\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    481\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39mregister_binary_elementwise_api\n\u001b[0;32m    482\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmultiply\u001b[39m(x, y, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    484\u001b[0m   \u001b[39m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[39m  For example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[39m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mmul(x, y, name)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6575\u001b[0m, in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6573\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   6574\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 6575\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   6576\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, y)\n\u001b[0;32m   6577\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   6578\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device(DEVICE):\n",
    "    train(50, 300, \"zombieEnvironment\", \"run1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a569b528fb1110d0d7d552dfd5bf7c0920d164754c3ee6d9fc5930b2e92fc65e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
