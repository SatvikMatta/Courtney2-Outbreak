{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGAI models (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based off of the pytorch tutorial [here](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html). It is intended to both create and train models for Courtney2-Outbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "import keras.models as models\n",
    "import keras\n",
    "from collections import namedtuple, Counter\n",
    "from queue import deque\n",
    "import random\n",
    "import math\n",
    "from typing import List\n",
    "from tqdm import tqdm  # used for progress meters\n",
    "\n",
    "sys.path.append(\"./\")  # make sure that it is able to import Board\n",
    "\n",
    "from Board import Board\n",
    "from constants import *\n",
    "from Player import ZombiePlayer, GovernmentPlayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"GPU\"\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "devices = tf.config.list_physical_devices(DEVICE)\n",
    "print(devices)\n",
    "if DEVICE == \"GPU\":\n",
    "    tf.config.experimental.set_memory_growth(devices[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZombieEnvironment:\n",
    "    ACTION_SPACE = tuple(range(8))\n",
    "    ACTION_MAPPINGS = {\n",
    "        0: \"moveUp\",\n",
    "        1: \"moveDown\",\n",
    "        2: \"moveLeft\",\n",
    "        3: \"moveRight\",\n",
    "        4: \"biteUp\",\n",
    "        5: \"biteDown\",\n",
    "        6: \"biteLeft\",\n",
    "        7: \"biteRight\",\n",
    "    }\n",
    "    SIZE = (6, 6)\n",
    "\n",
    "    def __init__(self, max_timesteps: int = 300, logdir: str = \"\", run_name=\"\") -> None:\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.reset()\n",
    "        self.total_timesteps = 0\n",
    "        self.total_invalid_moves = 0\n",
    "        self.writer = None\n",
    "        if logdir != \"\" and run_name != \"\":\n",
    "            self.writer = tf.summary.create_file_writer(f\"{logdir}/{run_name}\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = Board(ZombieEnvironment.SIZE, \"Zombie\")\n",
    "        self.board.populate(num_zombies=1)\n",
    "        self.enemyPlayer = GovernmentPlayer()\n",
    "        self.done = False\n",
    "\n",
    "        # coordinates of the first zombie\n",
    "        self.agentPosition = self.board.indexOf(True)\n",
    "\n",
    "        # useful for metrics\n",
    "        self.max_number_of_zombies = 1\n",
    "        self.episode_invalid_actions = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_timesteps = 0\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        action_name = ZombieEnvironment.ACTION_MAPPINGS[action]\n",
    "        if \"move\" in action_name:\n",
    "            valid, new_pos = self.board.actionToFunction[action_name](\n",
    "                self.board.toCoord(self.agentPosition)\n",
    "            )\n",
    "            if valid:\n",
    "                self.agentPosition = new_pos\n",
    "        else:  # bite variation\n",
    "            dest_coord = list(self.board.toCoord(self.agentPosition))\n",
    "            if action_name == \"biteUp\":\n",
    "                dest_coord[1] -= 1\n",
    "            elif action_name == \"biteDown\":\n",
    "                dest_coord[1] += 1\n",
    "            elif action_name == \"biteRight\":\n",
    "                dest_coord[0] += 1\n",
    "            else:\n",
    "                dest_coord[0] -= 1\n",
    "            valid, _ = self.board.actionToFunction[\"bite\"](dest_coord)\n",
    "\n",
    "        won = None\n",
    "        # do the opposing player's action if the action was valid.\n",
    "        if valid:\n",
    "            _action, coord = self.enemyPlayer.get_move(self.board)\n",
    "            if not _action:\n",
    "                self.done = True\n",
    "                won = True\n",
    "            else:\n",
    "                self.board.actionToFunction[_action](coord)\n",
    "            self.board.update()\n",
    "\n",
    "        # see if the game is over\n",
    "        if not self.board.States[\n",
    "            self.agentPosition\n",
    "        ].person.isZombie:  # zombie was cured\n",
    "            self.done = True\n",
    "            won = False\n",
    "        if not self.board.is_move_possible_at(self.agentPosition):  # no move possible\n",
    "            self.done = True\n",
    "        if self.episode_timesteps > self.max_timesteps:\n",
    "            self.done = True\n",
    "\n",
    "        # get obs, reward, done, info\n",
    "        obs, reward, done, info = (\n",
    "            self._get_obs(),\n",
    "            self._get_reward(action_name, valid, won),\n",
    "            self._get_done(),\n",
    "            self._get_info(),\n",
    "        )\n",
    "\n",
    "        # update the metrics\n",
    "        self.episode_reward += reward\n",
    "        if not valid:\n",
    "            self.episode_invalid_actions += 1\n",
    "            self.total_invalid_moves += 1\n",
    "        self.episode_timesteps += 1\n",
    "        self.max_number_of_zombies = max(\n",
    "            self.board.num_zombies(), self.max_number_of_zombies\n",
    "        )\n",
    "        self.total_timesteps += 1\n",
    "\n",
    "        # write the metrics\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"train/invalid_action_rate\",\n",
    "                    self.total_invalid_moves / self.total_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\"train/cur_reward\", reward, step=self.total_timesteps)\n",
    "\n",
    "        # return the obs, reward, done, info\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_done(self):\n",
    "        return self.done\n",
    "\n",
    "    def _get_reward(self, action_name: str, was_valid: bool, won: bool):\n",
    "        \"\"\"\n",
    "        Gonna try to return reward between [-1, 1]\n",
    "        This fits w/i tanh and sigmoid ranges\n",
    "        \"\"\"\n",
    "        if not was_valid:\n",
    "            return -0.5\n",
    "        if won is True:\n",
    "            return 1\n",
    "        if won is False:\n",
    "            return -1\n",
    "        if \"bite\" in action_name:\n",
    "            return 0.3\n",
    "        return 0.01  # this is the case where it was move\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Is based off the assumption that 4 is not in the returned board.\n",
    "        Uses 4 as the key for current position.\n",
    "        \"\"\"\n",
    "        ret = self.board.get_board()\n",
    "        ret[self.agentPosition] = 4\n",
    "        ret = np.array(ret, dtype=np.float32)\n",
    "        ret /= np.float32(4)\n",
    "        return ret\n",
    "\n",
    "    def render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.run(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def init_render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "\n",
    "        PF.initScreen(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        import pygame\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "    def write_run_metrics(self):\n",
    "        if self.writer is not None:\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/num_invalid_actions_per_ep\",\n",
    "                    self.episode_invalid_actions,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_length\",\n",
    "                    self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/episode_total_reward\",\n",
    "                    self.episode_reward,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/mean_reward\",\n",
    "                    self.episode_reward / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n",
    "                tf.summary.scalar(\n",
    "                    \"episode/percent_invalid_per_ep\",\n",
    "                    self.episode_invalid_actions / self.episode_timesteps,\n",
    "                    step=self.total_timesteps,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZOMBIE_OUTPUT_SIZE = len(ZombieEnvironment.ACTION_SPACE)\n",
    "INPUT_SHAPE = (ROWS * COLUMNS,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zombie_model():\n",
    "    \"\"\"\n",
    "    makes the model that will be used for zombies\n",
    "    The output of the model will be the predicted q value\n",
    "    for being in a certain state.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(INPUT_SHAPE))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(36 * 2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 4))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 8))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 16))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(36 * 32))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE * 16))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE * 8))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE * 4))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE * 2))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE, activation='tanh'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    zombie_policy = make_zombie_model()\n",
    "    zombie_target = make_zombie_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 36)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 36)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 72)                2664      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 72)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 144)               10512     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 144)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 288)               41760     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 288)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 576)               166464    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 576)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1152)              664704    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               147584    \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,044,688\n",
      "Trainable params: 1,044,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(zombie_policy.input_shape)\n",
    "zombie_policy.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "# make sure the output is correct shape\n",
    "with tf.device(DEVICE):\n",
    "    temp = zombie_policy(tf.random.normal((1, 36)), training=False)\n",
    "print(temp.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zombie_policy.load_weights(\"zombie_policy_weights\")\n",
    "# zombie_target.load_weights(\"zombie_policy_weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this acts as a class; useful in the training\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    optimizer = keras.optimizers.Adam(0.003)\n",
    "    loss = keras.losses.Huber()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.999\n",
    "EPSILON_MAX = 0.9  # exploration rate maximum\n",
    "EPSILON_MIN = 0.05  # exploration rate minimum\n",
    "EPS_DECAY = 1000  # decay rate, in steps\n",
    "TARGET_UPDATE = 10  # how many episodes before the target is updated\n",
    "\n",
    "BUFFER_CAPACITY = 10000\n",
    "memory = ReplayMemory(BUFFER_CAPACITY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_zombie_action(state, steps_done: int = -1):\n",
    "    \"\"\"\n",
    "    If no steps are provided, assuming not going to do\n",
    "    random exploration\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    eps_threshold = 0\n",
    "    if steps_done != -1:\n",
    "        eps_threshold = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN) * math.exp(\n",
    "            -1.0 * steps_done / EPS_DECAY\n",
    "        )\n",
    "    if sample > eps_threshold:\n",
    "        # Pick the action with the largest expected reward.\n",
    "        temp = zombie_policy(state, training=False)\n",
    "        numpy = temp.numpy().flatten()\n",
    "        return tf.constant([tuple(numpy).index(max(numpy))], dtype=tf.int32)\n",
    "    else:\n",
    "        return tf.constant([random.randrange(ZOMBIE_OUTPUT_SIZE)], dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_on_batch(\n",
    "    state_batch: tf.Tensor,\n",
    "    action_batch: tf.Tensor,\n",
    "    reward_batch: tf.Tensor,\n",
    "    non_final_next_states: tf.Tensor,\n",
    "    non_final_mask: tf.Tensor,\n",
    "):\n",
    "    with tf.GradientTape() as policy_tape:\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        action_batch = tf.expand_dims(action_batch, 1)\n",
    "        state_action_values = tf.gather_nd(\n",
    "            zombie_policy(state_batch, training=True), action_batch, 1\n",
    "        )\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = tf.scatter_nd(\n",
    "            tf.expand_dims(non_final_mask, 1),\n",
    "            tf.reduce_max(zombie_target(non_final_next_states, training=False), 1),\n",
    "            tf.constant([BATCH_SIZE]),\n",
    "        )\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = tf.squeeze(\n",
    "            (next_state_values * GAMMA) + reward_batch\n",
    "        )\n",
    "\n",
    "        # compute loss (mean squared error)\n",
    "        assert state_action_values.shape == expected_state_action_values.shape\n",
    "        _loss = loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    policy_gradient = policy_tape.gradient(_loss, zombie_policy.trainable_variables)\n",
    "\n",
    "    # apply gradient\n",
    "    optimizer.apply_gradients(zip(policy_gradient, zombie_policy.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, max_timesteps=200, render=False, logdir=\"\", run_name=\"\"):\n",
    "    env = ZombieEnvironment(max_timesteps, logdir, run_name)\n",
    "    if render:\n",
    "        env.init_render()\n",
    "\n",
    "    for episode in tqdm(range(epochs)):\n",
    "        # Initialize the environment and state\n",
    "        prev_obs = env.reset()\n",
    "        done = False\n",
    "        timesteps = 0\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_zombie_action(\n",
    "                tf.constant([prev_obs]), env.episode_timesteps\n",
    "            )\n",
    "            action = action.numpy()[0]  # \"flatten\" the tensor and take the item\n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            # reward = tf.constant([reward])\n",
    "\n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = new_obs\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(prev_obs, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            prev_obs = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "                # detailed explanation). This converts batch-array of Transitions\n",
    "                # to Transition of batch-arrays.\n",
    "                batch = Transition(*zip(*memory.sample(BATCH_SIZE)))\n",
    "\n",
    "                # compute the states that aren't terminal states\n",
    "                non_final_mask = tf.constant(\n",
    "                    tuple(\n",
    "                        idx\n",
    "                        for state, idx in zip(\n",
    "                            batch.next_state, range(len(batch.next_state))\n",
    "                        )\n",
    "                        if state is not None\n",
    "                    ),\n",
    "                )\n",
    "                non_final_next_states = tf.cast(\n",
    "                    tuple(state for state in batch.next_state if state is not None),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "\n",
    "                train_on_batch(\n",
    "                    tf.cast(batch.state, dtype=tf.float32),\n",
    "                    tf.cast(batch.action, dtype=tf.int32),\n",
    "                    tf.cast(batch.reward, dtype=tf.float32),\n",
    "                    non_final_next_states,\n",
    "                    non_final_mask,\n",
    "                )\n",
    "\n",
    "        env.write_run_metrics()\n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            zombie_policy.save_weights(\"zombie_policy_weights\")\n",
    "            zombie_target.load_weights(\"./zombie_policy_weights\")\n",
    "    env.close()\n",
    "    zombie_policy.save_weights(\"zombie_policy_weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NUMBER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 128/150 [11:20<01:57,  5.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Eliot\\Documents\\GitHub\\Courtney2-Outbreak\\SGAI_MK3\\model.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=1'>2</a>\u001b[0m     train(\u001b[39m150\u001b[39;49m, \u001b[39m500\u001b[39;49m, render\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, logdir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzombieEnvironment\u001b[39;49m\u001b[39m\"\u001b[39;49m, run_name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrun\u001b[39;49m\u001b[39m{\u001b[39;49;00mRUN_NUMBER\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=2'>3</a>\u001b[0m     RUN_NUMBER\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\Eliot\\Documents\\GitHub\\Courtney2-Outbreak\\SGAI_MK3\\model.ipynb Cell 27\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, max_timesteps, render, logdir, run_name)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=42'>43</a>\u001b[0m         non_final_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=43'>44</a>\u001b[0m             \u001b[39mtuple\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=44'>45</a>\u001b[0m                 idx\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=49'>50</a>\u001b[0m             ),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=50'>51</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=51'>52</a>\u001b[0m         non_final_next_states \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=52'>53</a>\u001b[0m             \u001b[39mtuple\u001b[39m(state \u001b[39mfor\u001b[39;00m state \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mnext_state \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=53'>54</a>\u001b[0m             dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=54'>55</a>\u001b[0m         )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=56'>57</a>\u001b[0m         train_on_batch(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=57'>58</a>\u001b[0m             tf\u001b[39m.\u001b[39;49mcast(batch\u001b[39m.\u001b[39;49mstate, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mfloat32),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=58'>59</a>\u001b[0m             tf\u001b[39m.\u001b[39;49mcast(batch\u001b[39m.\u001b[39;49maction, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mint32),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=59'>60</a>\u001b[0m             tf\u001b[39m.\u001b[39;49mcast(batch\u001b[39m.\u001b[39;49mreward, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mfloat32),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=60'>61</a>\u001b[0m             non_final_next_states,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=61'>62</a>\u001b[0m             non_final_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=62'>63</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=64'>65</a>\u001b[0m env\u001b[39m.\u001b[39mwrite_run_metrics()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/Courtney2-Outbreak/SGAI_MK3/model.ipynb#ch0000025?line=66'>67</a>\u001b[0m \u001b[39m# Update the target network, copying all weights and biases in DQN\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    train(150, 500, render=False, logdir=\"zombieEnvironment\", run_name=f\"run{RUN_NUMBER}\")\n",
    "    RUN_NUMBER+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_model(max_timesteps=200):\n",
    "    env = ZombieEnvironment(max_timesteps)\n",
    "    done = False\n",
    "    env.init_render()\n",
    "    obs = env.reset()\n",
    "    actions = []\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = select_zombie_action(tf.constant([obs])).numpy()[0]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "    env.close()\n",
    "    counter = Counter(actions)\n",
    "    print(counter.most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 202)]\n"
     ]
    }
   ],
   "source": [
    "watch_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a569b528fb1110d0d7d552dfd5bf7c0920d164754c3ee6d9fc5930b2e92fc65e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
