{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGAI models (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based off of the pytorch tutorial [here](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html). It is intended to both create and train models for Courtney2-Outbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sys.path.append(\"./\")  # make sure that it is able to import Board\n",
    "\n",
    "from Board import Board\n",
    "from constants import *\n",
    "from Player import ZombiePlayer, GovernmentPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"GPU\"\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "devices = tf.config.list_physical_devices(DEVICE)\n",
    "print(devices)\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZombieEnvironment:\n",
    "    ACTION_SPACE = tuple(range(8))\n",
    "    ACTION_MAPPINGS = {\n",
    "        0: \"moveUp\",\n",
    "        1: \"moveDown\",\n",
    "        2: \"moveLeft\",\n",
    "        3: \"moveRight\",\n",
    "        4: \"biteUp\",\n",
    "        5: \"biteDown\",\n",
    "        6: \"biteLeft\",\n",
    "        7: \"biteRight\",\n",
    "    }\n",
    "    SIZE = (6, 6)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = Board(ZombieEnvironment.SIZE, \"Zombie\")\n",
    "        self.board.populate(num_zombies=1)\n",
    "        self.enemyPlayer = GovernmentPlayer()\n",
    "        self.done = False\n",
    "\n",
    "        # coordinates of the first zombie\n",
    "        self.agentPosition = self.board.indexOf(True)\n",
    "        self.max_number_of_zombies = 1\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action:int):\n",
    "        action_name = ZombieEnvironment.ACTION_MAPPINGS[action]\n",
    "        if \"move\" in action_name:\n",
    "            valid, new_pos = self.board.actionToFunction[action_name](\n",
    "                self.board.toCoord(self.agentPosition)\n",
    "            )\n",
    "            if valid:\n",
    "                self.agentPosition = new_pos\n",
    "        else:  # bite variation\n",
    "            dest_coord = list(self.board.toCoord(self.agentPosition))\n",
    "            if action_name == \"biteUp\":\n",
    "                dest_coord[1] -= 1\n",
    "            elif action_name == \"biteDown\":\n",
    "                dest_coord[1] += 1\n",
    "            elif action_name == \"biteRight\":\n",
    "                dest_coord[0] +=1\n",
    "            else:\n",
    "                dest_coord[0] -= 1\n",
    "            valid, _ = self.board.actionToFunction[\"bite\"](dest_coord)\n",
    "\n",
    "        won = None\n",
    "        if valid:\n",
    "            # currently, if the computer chooses an invalid move, this doesn't\n",
    "            # end the environment. This can be changed.\n",
    "            _action, coord = self.enemyPlayer.get_move(self.board)\n",
    "            if not _action:\n",
    "                self.done = True\n",
    "                won = True\n",
    "            else:    \n",
    "                self.board.actionToFunction[_action](coord)\n",
    "            self.board.update()\n",
    "\n",
    "        if not self.board.States[self.agentPosition].person.isZombie:  # zombie was cured\n",
    "            self.done = True\n",
    "            won = False\n",
    "\n",
    "        self.max_number_of_zombies = max(self.board.num_zombies(), self.max_number_of_zombies)\n",
    "\n",
    "        return (\n",
    "            self._get_obs(),\n",
    "            self._get_reward(action_name, valid, won),\n",
    "            self._get_done(),\n",
    "            self._get_info(),\n",
    "        )\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_done(self):\n",
    "        return self.done\n",
    "\n",
    "    def _get_reward(self, action_name: str, was_valid: bool, won:bool):\n",
    "        if not was_valid:\n",
    "            return -100\n",
    "        if won is True:\n",
    "            return 100\n",
    "        if won is False:\n",
    "            return -100\n",
    "        if \"bite\" in action_name:\n",
    "            return 15\n",
    "        return 1  # this is the case where it was move\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array(self.board.get_board())\n",
    "\n",
    "    def render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "        PF.run(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def init_render(self):\n",
    "        import PygameFunctions as PF\n",
    "        import pygame\n",
    "        PF.initScreen(self.board)\n",
    "        pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        import pygame\n",
    "        pygame.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1 3 3 3 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "pygame 2.1.0 (SDL 2.0.16, Python 3.10.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "(array([2, 3, 1, 3, 3, 3, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), -100, False, {})\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "arr = tf.constant([1])\n",
    "env = ZombieEnvironment()\n",
    "\n",
    "print(env.reset())\n",
    "env.init_render()\n",
    "env.render()\n",
    "time.sleep(0.2)\n",
    "print(env.step(arr.numpy()[0]))\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as layers\n",
    "import keras.models as models\n",
    "import keras\n",
    "\n",
    "ZOMBIE_OUTPUT_SIZE = len(ZombieEnvironment.ACTION_SPACE)\n",
    "INPUT_SHAPE = (ROWS * COLUMNS,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that it is using the correct device\n",
    "with tf.device(DEVICE):\n",
    "    tf.random.normal((200, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zombie_model():\n",
    "    \"\"\"\n",
    "    makes the model that will be used for zombies\n",
    "    The output of the model will be the predicted q value\n",
    "    for being in a certain state.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(INPUT_SHAPE))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(128))  # 120 is arbitrary number\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(ZOMBIE_OUTPUT_SIZE))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    zombie_policy = make_zombie_model()\n",
    "    zombie_target = make_zombie_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 36)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 36)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2368      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,720\n",
      "Trainable params: 11,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(zombie_policy.input_shape)\n",
    "zombie_policy.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(DEVICE):\n",
    "    temp = zombie_policy(tf.random.normal((1, 36)), training=False)\n",
    "print(temp.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from queue import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this acts as a class; useful in the training\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    optimizer = keras.optimizers.Adam(0.004)\n",
    "    loss = keras.losses.MeanSquaredError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPSILON = 0.05  # exploration rate\n",
    "TARGET_UPDATE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_zombie_action(state):\n",
    "    sample = random.random()\n",
    "    if sample > EPSILON:\n",
    "        # t.max(1) will return largest column value of each row.\n",
    "        # second column on max result is index of where max element was\n",
    "        # found, so we pick action with the larger expected reward.\n",
    "        temp = zombie_policy(state, training=False)\n",
    "        numpy = temp.numpy().flatten()\n",
    "        return tf.constant([tuple(numpy).index(max(numpy))])\n",
    "    else:\n",
    "        return tf.constant([random.randrange(ZOMBIE_OUTPUT_SIZE)], dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def train_on_batch(_batch: List[Transition]):\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*_batch))\n",
    "\n",
    "    # compute the states that aren't terminal states\n",
    "    non_final_mask = tf.constant(\n",
    "        tuple(map(lambda state: state is not None, batch.next_state)), dtype=tf.bool\n",
    "    )\n",
    "    non_final_next_states = tf.constant([state for state in batch.next_state if state is not None])\n",
    "\n",
    "    state_batch = tf.constant(batch.state)\n",
    "    action_batch = tf.constant(batch.action)\n",
    "    reward_batch = tf.constant(batch.reward, dtype=tf.double)\n",
    "\n",
    "    with tf.GradientTape() as policy_tape:\n",
    "        state_action_values = zombie_policy(state_batch, training=True)\n",
    "        state_action_values = tf.reduce_max(state_action_values, 1)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = np.zeros(BATCH_SIZE)\n",
    "        next_state_values[non_final_mask.numpy()] = zombie_target(\n",
    "            non_final_next_states, training=False\n",
    "        ).numpy().max()\n",
    "        next_state_values = tf.constant(next_state_values)\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        expected_state_action_values = tf.expand_dims(expected_state_action_values, 1)  # equivalent of unsqueeze\n",
    "\n",
    "        # compute loss (mean squared error)\n",
    "        _loss = loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    policy_gradient = policy_tape.gradient(_loss, zombie_policy.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(policy_gradient, zombie_policy.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_CAPACITY = 10000\n",
    "memory = ReplayMemory(BUFFER_CAPACITY)\n",
    "\n",
    "\n",
    "def train(epochs, max_timesteps=200):\n",
    "    env = ZombieEnvironment()\n",
    "    env.init_render()\n",
    "    for i_episode in range(epochs):\n",
    "        # Initialize the environment and state\n",
    "        prev_obs = env.reset()\n",
    "        done = False\n",
    "        timesteps = 0\n",
    "        while not done:\n",
    "            timesteps += 1\n",
    "            env.render()\n",
    "            # Select and perform an action\n",
    "            action = select_zombie_action(tf.constant([prev_obs]))\n",
    "            action = action.numpy()[0]  # \"flatten\" the tensor and take the item\n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            # reward = tf.constant([reward])\n",
    "\n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = new_obs\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(prev_obs, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            prev_obs = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                train_on_batch(memory.sample(BATCH_SIZE))\n",
    "\n",
    "            if timesteps > max_timesteps:\n",
    "                continue\n",
    "\n",
    "        print(\"next episode\")\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            zombie_policy.save_weights(\"zombie_policy_weights\")\n",
    "            # zombie_target.load_state_dict(zombie_policy.state_dict())\n",
    "            zombie_target.load_weights(\"./zombie_policy_weights\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    train(50, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric evaluate how well its doing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a569b528fb1110d0d7d552dfd5bf7c0920d164754c3ee6d9fc5930b2e92fc65e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
